# -*- coding: utf-8 -*-
"""MMdetectionToPytorch

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1PujdC-vCAcixm7bplVmCkYWsgHFI6UgW
"""

import torch
import torchvision.transforms.functional as F
from mmdet.structures import DetDataSample
from mmengine.structures import InstanceData

def predict_cv(model, img, gt_bboxes, gt_labels):
  device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
  model.to(device)

  torch_img = F.pil_to_tensor(img)
  x = torch.stack([torch_img.to(device).float()])

  """
  The DetDataSample must follow this format:
  <DetDataSample(

      META INFORMATION
      img_shape: _
      scale_factor: _

      DATA FIELDS
      batch_input_shape: _
      gt_instances: <InstanceData(

              META INFORMATION

              DATA FIELDS
              bboxes: _
              labels: _
          ) at _>
  ) at _>
  """
  y = DetDataSample(metainfo={"img_shape": img.size, "scale_factor": (1, 1)})
  y.batch_input_shape = img.size

  gt_instances = InstanceData()
  gt_instances.bboxes = torch.tensor(gt_bboxes).to(device)
  gt_instances.labels = torch.tensor(gt_labels).to(device)

  y.gt_instances = gt_instances

  loss = model.loss(x, [y])

  return loss

"""
!pip install torchviz

!pip install -U openmim

!mim install mmengine
!mim install mmdet
!mim install mmcv

!mim download mmdet --config detr_r50_8xb2-150e_coco --dest ./checkpoints

from mmengine.utils import track_iter_progress
from mmdet.registry import VISUALIZERS
from mmdet.apis import init_detector, inference_detector


# Specify the path to model config and checkpoint file
config_file = '/content/config.py'
checkpoint_file = '/content/checkpoints/detr_r50_8xb2-150e_coco_20221023_153551-436d03e8.pth'

# Build the model from a config file and a checkpoint file
# Returns a nn.Module
model = init_detector(config_file, checkpoint_file, device='cuda:0')

import torch
device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")

model.to(device)

from PIL import Image
import torchvision.transforms.functional as F
from mmdet.structures import DetDataSample
from mmengine.structures import InstanceData

img = Image.open("/content/Grammar Jam 3 - Pranav Sambhu.png")
torch_img = F.pil_to_tensor(img)
x = torch.stack([torch_img.to(device).float()])

"""
The DetDataSample must follow this format:
<DetDataSample(

    META INFORMATION
    img_shape: _
    scale_factor: _

    DATA FIELDS
    batch_input_shape: _
    gt_instances: <InstanceData(

            META INFORMATION

            DATA FIELDS
            bboxes: _
            labels: _
        ) at _>
) at _>
"""
y = DetDataSample(metainfo={"img_shape": img.size, "scale_factor": (1, 1)})
y.batch_input_shape = img.size

gt_instances = InstanceData()
gt_instances.bboxes = torch.tensor([[1, 2, 200, 300]]).to(device)
gt_instances.labels = torch.tensor([0]).to(device)

y.gt_instances = gt_instances

loss = model.loss(x, [y])
loss

from torchviz import make_dot

make_dot(loss["loss_bbox"], params=dict(list(model.named_parameters()))).render("det_torchviz", format="png")
make_dot(loss["loss_cls"], params=dict(list(model.named_parameters()))).render("cls_torchviz", format="png")
make_dot(loss["loss_iou"], params=dict(list(model.named_parameters()))).render("iou_torchviz", format="png")

"""Check to see how the normalize the data when passing it in for prediction - for .predict it must be passed in as 0-255 in float becasue when it is passed in as 0-1 all the predicted bboxes are really small for .loss just look at the config file
Make sure the predicted bbox is in the format, xmin, ymin, xmax, ymax - yes
Find difference .forward and .predict (like grad_fn) - both work so ye

### Using mmdetection test pipeline and getting loss
"""

from mmcv.transforms import Compose

cfg = model.cfg

cfg = cfg.copy()

test_pipeline = model.cfg.test_dataloader.dataset['pipeline']

test_pipeline = Compose(test_pipeline)
test_pipeline

img_path = "/content/Grammar Jam 3 - Pranav Sambhu.png"

data_ = dict(img_path=img_path, img_id=0)

# build the data pipeline
data_ = test_pipeline(data_)

data_['inputs'] = [data_['inputs']]
data_['data_samples'] = [data_['data_samples']]

x = torch.stack([list(data_.values())[0][0].to(device).float()])
x

y = list(data_.values())[1][0]
y.batch_input_shape = (781, 1391)
y.gt_instances.bboxes = torch.tensor([[1, 2, 200, 300]]).to(device)
y.gt_instances.labels = torch.tensor([0]).to(device)
y

loss = model.loss(x, [y])
loss
"""
